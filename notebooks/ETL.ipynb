{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15b30c47",
   "metadata": {},
   "source": [
    "# SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "734b84ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "import  pyspark.sql.functions as F \n",
    "import numpy as np\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca48ae1",
   "metadata": {},
   "source": [
    "--Download jars manually besides main jars in image(image: jupyter/pyspark-notebook:spark-3.5.0) and put it in jars folder that spark,jupyter can access it\n",
    "-- in container must be in .....spark/external-jars  not ........spark/jars   to can overcome the overwrite\n",
    "-- Why we cannot download it in container ? because every time restart docker the all downloaded volatile(as memory)\n",
    "-- SO: to make some oprational files in container permanent must be in yaml file or in your local and container access it \n",
    "\n",
    "-- we install --> spark-excel_2.12-3.5.0_0.20.3.jar\n",
    "   and some dependencies --> poi-ooxml-5.2.5.jar\n",
    "                         --> commons-io-2.13.0.jar\n",
    "                         --> commons-collections4-4.4.jar\n",
    "\n",
    " ####### All This Jars to make Spark can read excel\n",
    "  ####### Another Solution: convert file on your loacl to csv      will be more effecience than excel                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70fd5584",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# get jars from local \n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ExcelRead\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.jars\", \"/usr/local/spark/external-jars/spark-excel_2.12-3.5.0_0.20.3.jar,\" \n",
    "                             \"/usr/local/spark/external-jars/poi-ooxml-5.2.5.jar,\"\n",
    "                             \"/usr/local/spark/external-jars/commons-collections4-4.4.jar,\"\n",
    "                             \"/usr/local/spark/external-jars/commons-io-2.13.0.jar\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02999399",
   "metadata": {},
   "source": [
    "####### get jars online (neeed downloaded every time)\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ExcelTest\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.crealytics:spark-excel_2.12:3.5.0_0.20.3\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "-- .config(\"spark.jars.packages\",<online resource>)\n",
    "-- .config(\"spark.jars\",<offline resource>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180b71f4",
   "metadata": {},
   "source": [
    "# SHOW FILES in SHARED FOLDER (OF CONTAINER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "db7b98b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os ## os mean container only ~ and we have folder(directory) in container called \"data\" (\"shared\" in our local machine)\n",
    "\n",
    "shared_path= \"/data\"\n",
    "\n",
    "\n",
    "files=[\"hotels_full_data_c2.xlsx\",\n",
    "\"hotels_reviews_data_v5.xlsx\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376dee04",
   "metadata": {},
   "source": [
    "# PIPILINE TO BUILD BRONZE LAYER "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "977546a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "‚è© File name : hotels_full_data_c2.xlsx\n",
      "==============================\n",
      "\n",
      "hotels_full_data_c2.xlsx Spark Read file .... ‚åõ\n",
      "hotels_full_data_c2.xlsx Successfully Readed ‚úÖ\n",
      "______________________________\n",
      "hotels_full_data_c2_bronze Store in HDFS .... ‚åõ\n",
      "hotels_full_data_c2_bronze Succefuuly Stored ‚úÖ\n",
      "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-\n",
      "\n",
      "==============================\n",
      "‚è© File name : hotels_reviews_data_v5.xlsx\n",
      "==============================\n",
      "\n",
      "hotels_reviews_data_v5.xlsx Spark Read file .... ‚åõ\n",
      "hotels_reviews_data_v5.xlsx Successfully Readed ‚úÖ\n",
      "______________________________\n",
      "hotels_reviews_data_v5_bronze Store in HDFS .... ‚åõ\n",
      "hotels_reviews_data_v5_bronze Succefuuly Stored ‚úÖ\n",
      "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for file_name in files:\n",
    "  print(\"=\"*30)\n",
    "  print(f\"‚è© File name : {file_name}\")\n",
    "  print(\"=\"*30)\n",
    "  print(\"\")\n",
    "  print(f\"{file_name } Spark Read file .... ‚åõ\")\n",
    "\n",
    "  df = spark.read \\\n",
    "    .format(\"com.crealytics.spark.excel\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(f\"{shared_path}/{file_name}\")\n",
    "  \n",
    "  print(f\"{file_name } Successfully Readed ‚úÖ\")\n",
    "  print(\"_\"*30)\n",
    "\n",
    "  file_name=file_name[0:-5]+'_bronze'\n",
    "\n",
    "  print(f\"{file_name } Store in HDFS .... ‚åõ\")\n",
    "\n",
    "  df.write.mode(\"overwrite\").parquet(f\"hdfs://namenode:9000/bronze/{file_name}.parquet\")\n",
    "  print(f\"{file_name} Succefuuly Stored ‚úÖ\")\n",
    "\n",
    "  print(\"_-\"*50)\n",
    "  print(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be11315",
   "metadata": {},
   "source": [
    "## PIPILINE TO BUILD SILVER LAYER "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3592a7a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+-------------+--------------------+--------------------+\n",
      "|          hotel_name|       reviewer|helpful_votes|                date|                text|\n",
      "+--------------------+---------------+-------------+--------------------+--------------------+\n",
      "|Reid's Palace, A ...|        David H|         NULL|David Hwrote a re...|A really special ...|\n",
      "|Reid's Palace, A ...|     AlisonY850|         71.0|AlisonY850wrote a...|A really special ...|\n",
      "|Reid's Palace, A ...|J. Kurt Schmidt|         NULL|J. Kurt Schmidtwr...|An outstanding st...|\n",
      "|Reid's Palace, A ...|      TonyS1956|         13.0|TonyS1956wrote a ...|In all respects, ...|\n",
      "|Reid's Palace, A ...|         Rose C|         31.0|Rose Cwrote a rev...|Our 1st time at R...|\n",
      "+--------------------+---------------+-------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- hotel_name: string (nullable = true)\n",
      " |-- reviewer: string (nullable = true)\n",
      " |-- helpful_votes: double (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_name=\"hotels_reviews_data_v1.xlsx\"\n",
    "df = spark.read \\\n",
    "    .format(\"com.crealytics.spark.excel\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(shared_path+'/'+file_name)\n",
    "\n",
    "df.show(5)\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0711536a",
   "metadata": {},
   "source": [
    "### hotel_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc00206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-\n",
      "‚û§ checking hotel_name....\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
      "\n",
      "1-Spacing....\n",
      "\n",
      "  Number of Spaced names : 0\n",
      "  NO FIXED...‚úÖ\n"
     ]
    }
   ],
   "source": [
    "print('_-'*50)\n",
    "print('‚û§ checking hotel_name....')\n",
    "print('-_'*50)\n",
    "print(\"\")\n",
    "print(\"1-Spacing....\")\n",
    "print(\"\")\n",
    "df_hotel_name = df.filter( F.col(\"hotel_name\") != F.trim(F.col(\"hotel_name\"))).select(\"hotel_name\")\n",
    "num=df_hotel_name.count()\n",
    "print(f\"  Number of Spaced names : {num}\")\n",
    "if(num==0):\n",
    "  print('  NO FIXED...‚úÖ')\n",
    "else:\n",
    "  df = df.withColumn(\"hotel_name\", F.trim(F.col(\"hotel_name\")))\n",
    "  print('  SPACING FIXED...‚úÖ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8daf3c",
   "metadata": {},
   "source": [
    "### reviewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e35170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-\n",
      "‚û§ checking reviewer....\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
      "\n",
      "1-Spacing....\n",
      "\n",
      "  Number of Spaced names : 0\n",
      "  NO FIXED...‚úÖ\n"
     ]
    }
   ],
   "source": [
    "print('_-'*50)\n",
    "print('‚û§ checking reviewer....')\n",
    "print('-_'*50)\n",
    "print(\"\")\n",
    "print(\"1-Spacing....\")\n",
    "print(\"\")\n",
    "df_reviewer = df.filter( F.col(\"reviewer\") != F.trim(F.col(\"reviewer\"))).select(\"reviewer\")\n",
    "num=df_reviewer.count()\n",
    "print(f\"  Number of Spaced names : {num}\")\n",
    "if(num==0):\n",
    "  print('  NO FIXED...‚úÖ')\n",
    "else:\n",
    "  df = df.withColumn(\"reviewer\", F.trim(F.col(\"reviewer\")))\n",
    "  print('  SPACEING FIXED...‚úÖ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03a110e",
   "metadata": {},
   "source": [
    "### helpful_votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8e0d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-\n",
      "‚û§ checking helpful_votes....\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
      "\n",
      "1-Casting....\n",
      "\n",
      "  Casting DONE ‚úÖ\n",
      "\n",
      "2-Check Values....\n",
      "  Number of Wrong Values : 4734\n",
      "  VALUES FIXED...‚úÖ\n"
     ]
    }
   ],
   "source": [
    "print('_-' * 50)\n",
    "print('‚û§ checking helpful_votes....')\n",
    "print('-_' * 50)\n",
    "print(\"\")\n",
    "print(\"1-Casting....\")\n",
    "print(\"\")\n",
    "\n",
    "df = df.withColumn(\"helpful_votes\", F.col(\"helpful_votes\").cast(\"int\"))\n",
    "print(\"  Casting DONE ‚úÖ\")\n",
    "print(\"\")\n",
    "print(\"2-Check Values....\")\n",
    "\n",
    "df_helpful_votes = df.filter(F.col(\"helpful_votes\").isNull() | (F.col(\"helpful_votes\") < 0))\n",
    "num = df_helpful_votes.count()\n",
    "print(f\"  Number of Wrong Values : {num}\")\n",
    "if num == 0:\n",
    "    print('  NO FIXED...‚úÖ')\n",
    "else:\n",
    "    df = df.withColumn(\n",
    "        \"helpful_votes\",\n",
    "        F.when((F.col(\"helpful_votes\").isNull()) | (F.col(\"helpful_votes\") < 0), 0)\n",
    "         .otherwise(F.col(\"helpful_votes\"))\n",
    "    )\n",
    "    print('  VALUES FIXED...‚úÖ')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ab7c49",
   "metadata": {},
   "source": [
    "### date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ffdf6b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-\n",
      "‚û§ Checking date ....\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
      "\n",
      "1-Checking for existing date values....\n",
      "  Found 16587 non-empty date values ‚Äî using main regex.\n",
      "\n",
      "2-Applying regex extraction....\n",
      "  Regex extraction DONE ‚úÖ\n",
      "\n",
      "3-Fixing missing months....\n",
      "  Missing month count: 8\n",
      "  Missing months fixed with default value 'Oct' ‚úÖ\n",
      "\n",
      "4-Filling missing years....\n",
      "  Missing years handled ‚úÖ\n",
      "\n",
      "5-Creating formatted date column....\n",
      "  Date column created ‚úÖ\n",
      "\n",
      "6-Sample preview:\n",
      "+-----------------------------------------+-----+----+--------------+\n",
      "|date                                     |month|year|extracted_date|\n",
      "+-----------------------------------------+-----+----+--------------+\n",
      "|David Hwrote a review Oct 17             |Oct  |2025|2025-10-01    |\n",
      "|AlisonY850wrote a review Oct 8           |Oct  |2025|2025-10-01    |\n",
      "|J. Kurt Schmidtwrote a review Oct 5      |Oct  |2025|2025-10-01    |\n",
      "|TonyS1956wrote a review Oct 5            |Oct  |2025|2025-10-01    |\n",
      "|Rose Cwrote a review Oct 3               |Oct  |2025|2025-10-01    |\n",
      "|MorewoodDudewrote a review Oct 2         |Oct  |2025|2025-10-01    |\n",
      "|William Turneywrote a review Oct 1       |Oct  |2025|2025-10-01    |\n",
      "|David Rwrote a review Oct 1              |Oct  |2025|2025-10-01    |\n",
      "|Curious24110066351wrote a review Sep 2025|Sep  |2025|2025-09-01    |\n",
      "|philipjohnwiggwrote a review Sep 2025    |Sep  |2025|2025-09-01    |\n",
      "+-----------------------------------------+-----+----+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('_-' * 50)\n",
    "print('‚û§ Checking date ....')\n",
    "print('-_' * 50)\n",
    "print(\"\")\n",
    "\n",
    "print(\"1-Checking for existing date values....\")\n",
    "date_count = df.filter(F.col(\"date\").isNotNull() & (F.length(F.col(\"date\")) > 0)).count()\n",
    "\n",
    "if date_count == 0:\n",
    "    print(\"  ‚ö†Ô∏è No date values found ‚Äî applying default regex pattern.\")\n",
    "    regex = r'\\b(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\\s*(\\d{1,2})?\\s*(\\d{4})?\\b'\n",
    "else:\n",
    "    print(f\"  Found {date_count} non-empty date values ‚Äî using main regex.\")\n",
    "    regex = r'\\b(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\\s*(\\d{1,2})?\\s*(\\d{4})?\\b'\n",
    "\n",
    "print(\"\")\n",
    "print(\"2-Applying regex extraction....\")\n",
    "df = df.withColumn(\"month\", F.regexp_extract(\"date\", regex, 1)) \\\n",
    "       .withColumn(\"year\", F.regexp_extract(\"date\", regex, 3))\n",
    "print(\"  Regex extraction DONE ‚úÖ\")\n",
    "\n",
    "print(\"\")\n",
    "print(\"3-Fixing missing months....\")\n",
    "missing_months = df.filter((F.col(\"month\") == \"\") | F.col(\"month\").isNull()).count()\n",
    "print(f\"  Missing month count: {missing_months}\")\n",
    "\n",
    "if missing_months > 0:\n",
    "    df = df.withColumn(\n",
    "        \"month\",\n",
    "        F.when((F.col(\"month\") == \"\") | F.col(\"month\").isNull(), \"Oct\").otherwise(F.col(\"month\"))\n",
    "    )\n",
    "    print(\"  Missing months fixed with default value 'Oct' ‚úÖ\")\n",
    "else:\n",
    "    print(\"  No missing months found ‚úÖ\")\n",
    "\n",
    "print(\"\")\n",
    "print(\"4-Filling missing years....\")\n",
    "df = df.withColumn(\"year\", F.when(F.col(\"year\") == \"\", \"2025\").otherwise(F.col(\"year\")))\n",
    "print(\"  Missing years handled ‚úÖ\")\n",
    "\n",
    "print(\"\")\n",
    "print(\"5-Creating formatted date column....\")\n",
    "df = df.withColumn(\"extracted_date_str\", F.concat_ws(\" \", F.col(\"month\"), F.col(\"year\"))) \\\n",
    "       .withColumn(\"extracted_date\",\n",
    "                   F.to_date(F.concat_ws(\"-\", F.lit(\"01\"), F.col(\"month\"), F.col(\"year\")),\n",
    "                             \"dd-MMM-yyyy\"))\n",
    "print(\"  Date column created ‚úÖ\")\n",
    "\n",
    "print(\"\")\n",
    "print(\"6-Sample preview:\")\n",
    "df.select(\"date\", \"month\", \"year\", \"extracted_date\").show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714b7fff",
   "metadata": {},
   "source": [
    "### month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5657f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-\n",
      "‚û§ Checking month ....\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
      "\n",
      "  Number of rows with NULL or empty month: 0\n",
      "  ‚úÖ No missing or blank month values found.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Number of distinct months found: 12\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m month_count \u001b[38;5;241m=\u001b[39m distinct_months\u001b[38;5;241m.\u001b[39mcount()\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Number of distinct months found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmonth_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 23\u001b[0m \u001b[43mdistinct_months\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Validate count\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m month_count \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m12\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py:972\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m    964\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    965\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    966\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    969\u001b[0m         },\n\u001b[1;32m    970\u001b[0m     )\n\u001b[0;32m--> 972\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mint_truncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream\u001b[38;5;241m.\u001b[39mreadline()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print('_-' * 50)\n",
    "print('‚û§ Checking month ....')\n",
    "print('-_' * 50)\n",
    "print(\"\")\n",
    "\n",
    "# Check for null or empty month values\n",
    "df_null_month = df.filter((F.col(\"month\").isNull()) | (F.trim(F.col(\"month\")) == \"\"))\n",
    "null_count = df_null_month.count()\n",
    "\n",
    "print(f\"  Number of rows with NULL or empty month: {null_count}\")\n",
    "if null_count > 0:\n",
    "    print(\"  ‚ö†Ô∏è Found missing or blank month values! ‚ùå\")\n",
    "    df_null_month.show()\n",
    "else:\n",
    "    print(\"  ‚úÖ No missing or blank month values found.\")\n",
    "\n",
    "print(\"\")\n",
    "#  Check distinct month values\n",
    "distinct_months = df.select(\"month\").distinct()\n",
    "month_count = distinct_months.count()\n",
    "\n",
    "print(f\"  Number of distinct months found: {month_count}\")\n",
    "distinct_months.show(20, truncate=False)\n",
    "\n",
    "# Validate count\n",
    "if month_count > 12:\n",
    "    print(\"  ‚ö†Ô∏è ERROR: More than 12 distinct months found (possible blank or invalid entry)! ‚ùå\")\n",
    "else:\n",
    "    print(\"  ‚úÖ Month values are valid (‚â§ 12).\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281c722b",
   "metadata": {},
   "source": [
    "### year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef205e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-\n",
      "‚û§ Checking year....\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
      "\n",
      "  Number of rows with NULL or empty year: 0\n",
      "  ‚úÖ No missing or blank year values found.\n",
      "\n",
      "  Number of invalid years (negative or > 2025): 0\n",
      "  ‚úÖ All year values are valid (0 ‚â§ year ‚â§ 2025).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('_-' * 50)\n",
    "print('‚û§ Checking year....')\n",
    "print('-_' * 50)\n",
    "print(\"\")\n",
    "\n",
    "# Convert year to integer\n",
    "df = df.withColumn(\"year_int\", F.col(\"year\").cast(\"int\"))\n",
    "\n",
    "# Check for null or invalid (non-numeric) years\n",
    "df_null_year = df.filter(F.col(\"year\").isNull() | (F.trim(F.col(\"year\")) == \"\"))\n",
    "null_year_count = df_null_year.count()\n",
    "print(f\"  Number of rows with NULL or empty year: {null_year_count}\")\n",
    "\n",
    "if null_year_count > 0:\n",
    "    print(\"  ‚ö†Ô∏è Found missing or blank year values! ‚ùå\")\n",
    "    df_null_year.show()\n",
    "else:\n",
    "    print(\"  ‚úÖ No missing or blank year values found.\")\n",
    "\n",
    "print(\"\")\n",
    "# Check for out-of-range years (negative or > 2025)\n",
    "df_invalid_years = df.filter((F.col(\"year_int\") < 0) | (F.col(\"year_int\") > 2025))\n",
    "invalid_year_count = df_invalid_years.count()\n",
    "\n",
    "print(f\"  Number of invalid years (negative or > 2025): {invalid_year_count}\")\n",
    "if invalid_year_count > 0:\n",
    "    print(\"  ‚ö†Ô∏è Found invalid year values! Fixing them to 2025...\")\n",
    "    df = df.withColumn(\n",
    "        \"year\",\n",
    "        F.when((F.col(\"year_int\") < 0) | (F.col(\"year_int\") > 2025) | F.col(\"year_int\").isNull(),\n",
    "               \"2025\").otherwise(F.col(\"year\"))\n",
    "    )\n",
    "    print(\"  ‚úÖ Invalid years fixed to 2025.\")\n",
    "else:\n",
    "    print(\"  ‚úÖ All year values are valid (0 ‚â§ year ‚â§ 2025).\")\n",
    "\n",
    "print(\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051c3c39",
   "metadata": {},
   "source": [
    "## REVIEWS PIPELINE (CLEANING,TRANSFORMATION,TRACKING TIME,..)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "71e747fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Utility function for timing\n",
    "def log_time(stage_name, func):\n",
    "    print('_-'*50)\n",
    "    print(f'‚û§ {stage_name}....')\n",
    "    print('-_'*50)\n",
    "    start_time = time.time()\n",
    "    print(f\"‚è±Ô∏è  Start time: {datetime.now().strftime('%H:%M:%S')}\")\n",
    "    print(\"\")\n",
    "\n",
    "    try:\n",
    "        func()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in {stage_name}: {e}\")\n",
    "    finally:\n",
    "        end_time = time.time()\n",
    "        duration = round(end_time - start_time, 2)\n",
    "        print(f\"\\n‚úÖ End time: {datetime.now().strftime('%H:%M:%S')}\")\n",
    "        print(f\"üïí Duration: {duration} seconds\")\n",
    "        print('_-'*50 + \"\\n\")\n",
    "        \n",
    "\n",
    "# ---------------- STAGE 1 ----------------\n",
    "def check_hotel_name():\n",
    "    try:\n",
    "        print(\"1-Spacing....\\n\")\n",
    "        df_hotel_name = df.filter(F.col(\"hotel_name\") != F.trim(F.col(\"hotel_name\"))).select(\"hotel_name\")\n",
    "        num = df_hotel_name.count()\n",
    "        print(f\"  Number of Spaced names : {num}\")\n",
    "        if num == 0:\n",
    "            print('  NO FIXED...‚úÖ')\n",
    "        else:\n",
    "            globals()['df'] = df.withColumn(\"hotel_name\", F.trim(F.col(\"hotel_name\")))\n",
    "            print('  SPACING FIXED...‚úÖ')\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Exception in hotel_name stage: {e}\")\n",
    "\n",
    "# ---------------- STAGE 2 ----------------\n",
    "def check_reviewer():\n",
    "    try:\n",
    "        print(\"1-Spacing....\\n\")\n",
    "        df_reviewer = df.filter(F.col(\"reviewer\") != F.trim(F.col(\"reviewer\"))).select(\"reviewer\")\n",
    "        num = df_reviewer.count()\n",
    "        print(f\"  Number of Spaced names : {num}\")\n",
    "        if num == 0:\n",
    "            print('  NO FIXED...‚úÖ')\n",
    "        else:\n",
    "            globals()['df'] = df.withColumn(\"reviewer\", F.trim(F.col(\"reviewer\")))\n",
    "            print('  SPACING FIXED...‚úÖ')\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Exception in reviewer stage: {e}\")\n",
    "\n",
    "# ---------------- STAGE 3 ----------------\n",
    "def check_helpful_votes():\n",
    "    try:\n",
    "        print(\"1-Casting....\\n\")\n",
    "        globals()['df'] = df.withColumn(\"helpful_votes\", F.col(\"helpful_votes\").cast(\"int\"))\n",
    "        print(\"  Casting DONE ‚úÖ\\n\")\n",
    "\n",
    "        print(\"2-Check Values....\")\n",
    "        df_helpful_votes = df.filter(F.col(\"helpful_votes\").isNull() | (F.col(\"helpful_votes\") < 0))\n",
    "        num = df_helpful_votes.count()\n",
    "        print(f\"  Number of Wrong Values : {num}\")\n",
    "        if num == 0:\n",
    "            print('  NO FIXED...‚úÖ')\n",
    "        else:\n",
    "            globals()['df'] = df.withColumn(\n",
    "                \"helpful_votes\",\n",
    "                F.when((F.col(\"helpful_votes\").isNull()) | (F.col(\"helpful_votes\") < 0), 0)\n",
    "                 .otherwise(F.col(\"helpful_votes\"))\n",
    "            )\n",
    "            print('  VALUES FIXED...‚úÖ')\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Exception in helpful_votes stage: {e}\")\n",
    "\n",
    "# ---------------- STAGE 4 ----------------\n",
    "def check_date():\n",
    "    try:\n",
    "        print(\"1-Checking for existing date values....\")\n",
    "        date_count = df.filter(F.col(\"date\").isNotNull() & (F.length(F.col(\"date\")) > 0)).count()\n",
    "\n",
    "        if date_count == 0:\n",
    "            print(\"  ‚ö†Ô∏è No date values found ‚Äî applying default regex pattern.\")\n",
    "            regex = r'\\b(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\\s*(\\d{1,2})?\\s*(\\d{4})?\\b'\n",
    "        else:\n",
    "            print(f\"  Found {date_count} non-empty date values ‚Äî using main regex.\")\n",
    "            regex = r'\\b(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\\s*(\\d{1,2})?\\s*(\\d{4})?\\b'\n",
    "\n",
    "        print(\"\\n2-Applying regex extraction....\")\n",
    "        globals()['df'] = df.withColumn(\"month\", F.regexp_extract(\"date\", regex, 1)) \\\n",
    "                           .withColumn(\"year\", F.regexp_extract(\"date\", regex, 3))\n",
    "        print(\"  Regex extraction DONE ‚úÖ\")\n",
    "\n",
    "        print(\"\\n3-Fixing missing months....\")\n",
    "        missing_months = df.filter((F.col(\"month\") == \"\") | F.col(\"month\").isNull()).count()\n",
    "        print(f\"  Missing month count: {missing_months}\")\n",
    "        if missing_months > 0:\n",
    "            globals()['df'] = df.withColumn(\n",
    "                \"month\",\n",
    "                F.when((F.col(\"month\") == \"\") | F.col(\"month\").isNull(), \"Oct\").otherwise(F.col(\"month\"))\n",
    "            )\n",
    "            print(\"  Missing months fixed with default value 'Oct' ‚úÖ\")\n",
    "        else:\n",
    "            print(\"  No missing months found ‚úÖ\")\n",
    "\n",
    "        print(\"\\n4-Filling missing years....\")\n",
    "        globals()['df'] = df.withColumn(\"year\", F.when(F.col(\"year\") == \"\", \"2025\").otherwise(F.col(\"year\")))\n",
    "        print(\"  Missing years handled ‚úÖ\")\n",
    "\n",
    "        print(\"\\n5-Creating formatted date column....\")\n",
    "        globals()['df'] = df.withColumn(\"extracted_date_str\", F.concat_ws(\" \", F.col(\"month\"), F.col(\"year\"))) \\\n",
    "                           .withColumn(\"extracted_date\",\n",
    "                                       F.to_date(F.concat_ws(\"-\", F.lit(\"01\"), F.col(\"month\"), F.col(\"year\")),\n",
    "                                                 \"dd-MMM-yyyy\"))\n",
    "        print(\"  Date column created ‚úÖ\")\n",
    "\n",
    "        \n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Exception in date stage: {e}\")\n",
    "\n",
    "# ---------------- STAGE 5 ----------------\n",
    "def check_month():\n",
    "    try:\n",
    "        print(\"1-Checking null or empty months....\\n\")\n",
    "        df_null_month = df.filter((F.col(\"month\").isNull()) | (F.trim(F.col(\"month\")) == \"\"))\n",
    "        null_count = df_null_month.count()\n",
    "        print(f\"  Number of rows with NULL or empty month: {null_count}\")\n",
    "        if null_count > 0:\n",
    "            print(\"  ‚ö†Ô∏è Found missing or blank month values! ‚ùå\")\n",
    "            df_null_month.show()\n",
    "        else:\n",
    "            print(\"  ‚úÖ No missing or blank month values found.\")\n",
    "\n",
    "        print(\"\\n2-Checking distinct months....\")\n",
    "        distinct_months = df.select(\"month\").distinct()\n",
    "        month_count = distinct_months.count()\n",
    "        print(f\"  Number of distinct months found: {month_count}\")\n",
    "        distinct_months.show(20, truncate=False)\n",
    "\n",
    "        if month_count > 12:\n",
    "            print(\"  ‚ö†Ô∏è ERROR: More than 12 distinct months found! ‚ùå\")\n",
    "        else:\n",
    "            print(\"  ‚úÖ Month values are valid (‚â§ 12).\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Exception in month stage: {e}\")\n",
    "\n",
    "# ---------------- STAGE 6 ----------------\n",
    "def check_year():\n",
    "    try:\n",
    "        print(\"1-Casting and validation....\\n\")\n",
    "        globals()['df'] = df.withColumn(\"year_int\", F.col(\"year\").cast(\"int\"))\n",
    "\n",
    "        df_null_year = df.filter(F.col(\"year\").isNull() | (F.trim(F.col(\"year\")) == \"\"))\n",
    "        null_year_count = df_null_year.count()\n",
    "        print(f\"  Number of rows with NULL or empty year: {null_year_count}\")\n",
    "        if null_year_count > 0:\n",
    "            print(\"  ‚ö†Ô∏è Found missing or blank year values! ‚ùå\")\n",
    "            df_null_year.show()\n",
    "        else:\n",
    "            print(\"  ‚úÖ No missing or blank year values found.\")\n",
    "\n",
    "        print(\"\\n2-Checking invalid year range....\")\n",
    "        df_invalid_years = df.filter((F.col(\"year_int\") < 0) | (F.col(\"year_int\") > 2025))\n",
    "        invalid_year_count = df_invalid_years.count()\n",
    "        print(f\"  Number of invalid years (negative or > 2025): {invalid_year_count}\")\n",
    "        if invalid_year_count > 0:\n",
    "            print(\"  ‚ö†Ô∏è Found invalid year values! Fixing them to 2025...\")\n",
    "            globals()['df'] = df.withColumn(\n",
    "                \"year\",\n",
    "                F.when((F.col(\"year_int\") < 0) | (F.col(\"year_int\") > 2025) | F.col(\"year_int\").isNull(),\n",
    "                       \"2025\").otherwise(F.col(\"year\"))\n",
    "            )\n",
    "            print(\"  ‚úÖ Invalid years fixed to 2025.\")\n",
    "        else:\n",
    "            print(\"  ‚úÖ All year values are valid (0 ‚â§ year ‚â§ 2025).\")\n",
    "\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Exception in year stage: {e}\")\n",
    "\n",
    "\n",
    "# ======= RUN STAGES WITH TIMING =======\n",
    "#log_time(\"Checking hotel_name\", check_hotel_name)\n",
    "#log_time(\"Checking reviewer\", check_reviewer)\n",
    "#log_time(\"Checking helpful_votes\", check_helpful_votes)\n",
    "#log_time(\"Checking date\", check_date)\n",
    "#log_time(\"Checking month\", check_month)\n",
    "#log_time(\"Checking year\", check_year)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0336e3",
   "metadata": {},
   "source": [
    "## UPLOAD in HDFS SILVER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ba7af242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "‚è© File name : hotels_reviews_data_v5.xlsx\n",
      "==============================\n",
      "\n",
      "hotels_reviews_data_v5.xlsx Spark Read file .... ‚åõ\n",
      "hotels_reviews_data_v5.xlsx Spark Read DONE ‚úÖ\n",
      "\n",
      "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-\n",
      "‚û§ Checking hotel_name....\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
      "‚è±Ô∏è  Start time: 21:49:18\n",
      "\n",
      "1-Spacing....\n",
      "\n",
      "  Number of Spaced names : 0\n",
      "  NO FIXED...‚úÖ\n",
      "\n",
      "‚úÖ End time: 21:49:18\n",
      "üïí Duration: 0.53 seconds\n",
      "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-\n",
      "\n",
      "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-\n",
      "‚û§ Checking reviewer....\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
      "‚è±Ô∏è  Start time: 21:49:18\n",
      "\n",
      "1-Spacing....\n",
      "\n",
      "  Number of Spaced names : 0\n",
      "  NO FIXED...‚úÖ\n",
      "\n",
      "‚úÖ End time: 21:49:19\n",
      "üïí Duration: 0.51 seconds\n",
      "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-\n",
      "\n",
      "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-\n",
      "‚û§ Checking helpful_votes....\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
      "‚è±Ô∏è  Start time: 21:49:19\n",
      "\n",
      "1-Casting....\n",
      "\n",
      "  Casting DONE ‚úÖ\n",
      "\n",
      "2-Check Values....\n",
      "  Number of Wrong Values : 2118\n",
      "  VALUES FIXED...‚úÖ\n",
      "\n",
      "‚úÖ End time: 21:49:19\n",
      "üïí Duration: 0.41 seconds\n",
      "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-\n",
      "\n",
      "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-\n",
      "‚û§ Checking date....\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
      "‚è±Ô∏è  Start time: 21:49:19\n",
      "\n",
      "1-Checking for existing date values....\n",
      "  Found 8258 non-empty date values ‚Äî using main regex.\n",
      "\n",
      "2-Applying regex extraction....\n",
      "  Regex extraction DONE ‚úÖ\n",
      "\n",
      "3-Fixing missing months....\n",
      "  Missing month count: 0\n",
      "  No missing months found ‚úÖ\n",
      "\n",
      "4-Filling missing years....\n",
      "  Missing years handled ‚úÖ\n",
      "\n",
      "5-Creating formatted date column....\n",
      "  Date column created ‚úÖ\n",
      "\n",
      "‚úÖ End time: 21:49:20\n",
      "üïí Duration: 0.96 seconds\n",
      "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-\n",
      "\n",
      "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-\n",
      "‚û§ Checking month....\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
      "‚è±Ô∏è  Start time: 21:49:20\n",
      "\n",
      "1-Checking null or empty months....\n",
      "\n",
      "  Number of rows with NULL or empty month: 0\n",
      "  ‚úÖ No missing or blank month values found.\n",
      "\n",
      "2-Checking distinct months....\n",
      "  Number of distinct months found: 12\n",
      "+-----+\n",
      "|month|\n",
      "+-----+\n",
      "|Oct  |\n",
      "|Sep  |\n",
      "|Dec  |\n",
      "|Aug  |\n",
      "|May  |\n",
      "|Jun  |\n",
      "|Feb  |\n",
      "|Nov  |\n",
      "|Mar  |\n",
      "|Jan  |\n",
      "|Apr  |\n",
      "|Jul  |\n",
      "+-----+\n",
      "\n",
      "  ‚úÖ Month values are valid (‚â§ 12).\n",
      "\n",
      "‚úÖ End time: 21:49:22\n",
      "üïí Duration: 1.59 seconds\n",
      "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-\n",
      "\n",
      "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-\n",
      "‚û§ Checking year....\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
      "‚è±Ô∏è  Start time: 21:49:22\n",
      "\n",
      "1-Casting and validation....\n",
      "\n",
      "  Number of rows with NULL or empty year: 0\n",
      "  ‚úÖ No missing or blank year values found.\n",
      "\n",
      "2-Checking invalid year range....\n",
      "  Number of invalid years (negative or > 2025): 0\n",
      "  ‚úÖ All year values are valid (0 ‚â§ year ‚â§ 2025).\n",
      "\n",
      "‚úÖ End time: 21:49:23\n",
      "üïí Duration: 1.03 seconds\n",
      "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-\n",
      "\n",
      "hotels_reviews_data_v5.xlsx Successfully Processed ‚úÖ\n",
      "______________________________\n",
      "hotels_reviews_data_v5_silver Store in HDFS .... ‚åõ\n",
      "hotels_reviews_data_v5_silver Successfully Stored ‚úÖ\n",
      "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for file_name in files:\n",
    "\n",
    "    if 'reviews' in file_name:\n",
    "        print(\"=\"*30)\n",
    "        print(f\"‚è© File name : {file_name}\")\n",
    "        print(\"=\"*30)\n",
    "        print(\"\")\n",
    "\n",
    "        try:\n",
    "            print(f\"{file_name} Spark Read file .... ‚åõ\")\n",
    "            df = spark.read \\\n",
    "                .format(\"com.crealytics.spark.excel\") \\\n",
    "                .option(\"header\", \"true\") \\\n",
    "                .option(\"inferSchema\", \"true\") \\\n",
    "                .load(shared_path + '/' + file_name)\n",
    "            print(f\"{file_name} Spark Read DONE ‚úÖ\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error reading file {file_name}: {e}\")\n",
    "            continue  # skip this file if reading fails\n",
    "\n",
    "        # ======= Run cleaning/validation stages with timing =======\n",
    "        log_time(\"Checking hotel_name\", check_hotel_name)\n",
    "        log_time(\"Checking reviewer\", check_reviewer)\n",
    "        log_time(\"Checking helpful_votes\", check_helpful_votes)\n",
    "        log_time(\"Checking date\", check_date)\n",
    "        log_time(\"Checking month\", check_month)\n",
    "        log_time(\"Checking year\", check_year)\n",
    "\n",
    "        print(f\"{file_name} Successfully Processed ‚úÖ\")\n",
    "        print(\"_\"*30)\n",
    "\n",
    "        # Prepare silver file name\n",
    "        silver_file_name = file_name[0:-5] + '_silver'\n",
    "\n",
    "        try:\n",
    "            print(f\"{silver_file_name} Store in HDFS .... ‚åõ\")\n",
    "            df.write.mode(\"overwrite\").parquet(f\"hdfs://namenode:9000/silver/{silver_file_name}.parquet\")\n",
    "            print(f\"{silver_file_name} Successfully Stored ‚úÖ\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error writing {silver_file_name} to HDFS: {e}\")\n",
    "\n",
    "        print(\"_-\"*50)\n",
    "        print(\"\")\n",
    "\n",
    "\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7c970e",
   "metadata": {},
   "source": [
    "## Hotels PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9647e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name=\"hotels_full_data_c1.xlsx\"\n",
    "df = spark.read \\\n",
    "    .format(\"com.crealytics.spark.excel\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(shared_path+'/'+file_name)\n",
    "\n",
    "df.show(5)\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac50d582",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('_-'*50)\n",
    "print('‚û§ checking hotel_name....')\n",
    "print('-_'*50)\n",
    "print(\"\")\n",
    "print(\"1-Spacing....\")\n",
    "print(\"\")\n",
    "df_hotel_name = df.filter( F.col(\"name\") != F.trim(F.col(\"name\"))).select(\"name\")\n",
    "num=df_hotel_name.count()\n",
    "print(f\"  Number of Spaced names : {num}\")\n",
    "if(num==0):\n",
    "  print('  NO FIXED...‚úÖ')\n",
    "else:\n",
    "  df = df.withColumn(\"name\", F.trim(F.col(\"name\")))\n",
    "  print('  SPACING FIXED...‚úÖ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f665ef7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace, concat, lit, col\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"gps_link\",\n",
    "    concat(\n",
    "        lit(\"https://www.google.com/maps/search/?api=1&query=\"),\n",
    "        regexp_replace(col(\"location\"), \" \", \"+\")\n",
    "    )\n",
    ")\n",
    "\n",
    "df.select(\"location\", \"gps_link\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6226b8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, explode, trim, col\n",
    "\n",
    "# Split the comma-separated amenities into an array\n",
    "df_split = df.withColumn(\"amenity\", explode(split(col(\"amenities\"), \",\")))\n",
    "\n",
    "# Trim spaces\n",
    "df_split = df_split.withColumn(\"amenity\", trim(col(\"amenity\")))\n",
    "\n",
    "# Get distinct amenities\n",
    "unique_amenities = df_split.select(\"amenity\").distinct()\n",
    "\n",
    "# Collect to Python list\n",
    "amenities_list = [row[\"amenity\"] for row in unique_amenities.collect()]\n",
    "\n",
    "print(len(amenities_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fb3cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "for amenity in amenities_list:\n",
    "    clean_name = amenity.lower().replace(\" \", \"_\").replace(\"/\", \"_\").replace(\"'\", \"\").replace(\"(\", \"\").replace(\")\", \"\")\n",
    "    df = df.withColumn(\n",
    "        clean_name,\n",
    "        col(\"amenities\").contains(amenity).cast(\"int\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9505212c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba13246",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rating = df.filter(\n",
    "   (col(\"rating\") < 0) | (col(\"rating\") > 5)\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772404b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_price = df.filter((col(\"price\") < 0)).select(\"name\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f5bad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, col\n",
    "\n",
    "# Split the images column by comma\n",
    "df = df.withColumn(\"images_array\", split(col(\"images\"), \",\"))\n",
    "\n",
    "# Create 5 new columns from the array elements\n",
    "for i in range(5):\n",
    "    df = df.withColumn(f\"image{i+1}\", col(\"images_array\")[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0c4fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"image1\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a4077f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function for timing\n",
    "def log_time(stage_name, func):\n",
    "    print('_-'*50)\n",
    "    print(f'‚û§ {stage_name}....')\n",
    "    print('-_'*50)\n",
    "    start_time = time.time()\n",
    "    print(f\"‚è±Ô∏è  Start time: {datetime.now().strftime('%H:%M:%S')}\")\n",
    "    print(\"\")\n",
    "\n",
    "    try:\n",
    "        func()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in {stage_name}: {e}\")\n",
    "    finally:\n",
    "        end_time = time.time()\n",
    "        duration = round(end_time - start_time, 2)\n",
    "        print(f\"\\n‚úÖ End time: {datetime.now().strftime('%H:%M:%S')}\")\n",
    "        print(f\"üïí Duration: {duration} seconds\")\n",
    "        print('_-'*50 + \"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# ---------------- STAGE 1 ----------------\n",
    "def check_hotel_name():\n",
    "    try:\n",
    "        print('_-' * 50)\n",
    "        print('‚û§ Checking hotel_name....')\n",
    "        print('-_' * 50)\n",
    "        print(\"\")\n",
    "        print(\"1-Spacing....\")\n",
    "        print(\"\")\n",
    "\n",
    "        df_hotel_name = df.filter(F.col(\"name\") != F.trim(F.col(\"name\"))).select(\"name\")\n",
    "        num = df_hotel_name.count()\n",
    "        print(f\"  Number of Spaced names : {num}\")\n",
    "        if num == 0:\n",
    "            print('  NO FIXED...‚úÖ')\n",
    "        else:\n",
    "            globals()['df'] = df.withColumn(\"name\", F.trim(F.col(\"name\")))\n",
    "            print('  SPACING FIXED...‚úÖ')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Exception in hotel_name stage: {e}\")\n",
    "\n",
    "\n",
    "# ---------------- STAGE 2 ----------------\n",
    "def create_gps_link():\n",
    "    try:\n",
    "        print('_-' * 50)\n",
    "        print('‚û§ Creating GPS link for each location....')\n",
    "        print('-_' * 50)\n",
    "        print(\"\")\n",
    "\n",
    "        globals()['df'] = df.withColumn(\n",
    "            \"gps_link\",\n",
    "            F.concat(\n",
    "                F.lit(\"https://www.google.com/maps/search/?api=1&query=\"),\n",
    "                F.regexp_replace(F.col(\"location\"), \" \", \"+\")\n",
    "            )\n",
    "        )\n",
    "\n",
    "        print(\"  GPS link column created successfully ‚úÖ\")\n",
    "        print(\"  Sample links:\")\n",
    "        df.select(\"location\", \"gps_link\").show(5, truncate=False)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Exception in GPS link stage: {e}\")\n",
    "\n",
    "\n",
    "# ---------------- STAGE 3 ----------------\n",
    "def extract_amenities():\n",
    "    try:\n",
    "        print('_-' * 50)\n",
    "        print('‚û§ Extracting amenities....')\n",
    "        print('-_' * 50)\n",
    "        print(\"\")\n",
    "\n",
    "        df_split = df.withColumn(\"amenity\", F.explode(F.split(F.col(\"amenities\"), \",\")))\n",
    "        df_split = df_split.withColumn(\"amenity\", F.trim(F.col(\"amenity\")))\n",
    "\n",
    "        unique_amenities = df_split.select(\"amenity\").distinct()\n",
    "        amenities_list = [row[\"amenity\"] for row in unique_amenities.collect()]\n",
    "        print(f\"  Total unique amenities found: {len(amenities_list)}\")\n",
    "\n",
    "        for amenity in amenities_list:\n",
    "            clean_name = (\n",
    "                amenity.lower()\n",
    "                .replace(\" \", \"_\")\n",
    "                .replace(\"/\", \"_\")\n",
    "                .replace(\"'\", \"\")\n",
    "                .replace(\"(\", \"\")\n",
    "                .replace(\")\", \"\")\n",
    "            )\n",
    "            globals()['df'] = df.withColumn(\n",
    "                clean_name,\n",
    "                F.col(\"amenities\").contains(amenity).cast(\"int\")\n",
    "            )\n",
    "\n",
    "        print(\"  Amenity columns created successfully ‚úÖ\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Exception in amenities stage: {e}\")\n",
    "\n",
    "\n",
    "# ---------------- STAGE 4 ----------------\n",
    "def check_rating():\n",
    "    try:\n",
    "        print('_-' * 50)\n",
    "        print('‚û§ Checking rating values....')\n",
    "        print('-_' * 50)\n",
    "        print(\"\")\n",
    "\n",
    "        df_invalid = df.filter((F.col(\"rating\") < 0) | (F.col(\"rating\") > 5))\n",
    "        invalid_count = df_invalid.count()\n",
    "        print(f\"  Number of invalid ratings (<0 or >5): {invalid_count}\")\n",
    "\n",
    "        if invalid_count == 0:\n",
    "            print(\"  All ratings are valid ‚úÖ\")\n",
    "        else:\n",
    "            globals()['df'] = df.withColumn(\n",
    "                \"rating\",\n",
    "                F.when((F.col(\"rating\") < 0) | (F.col(\"rating\") > 5), 0)\n",
    "                 .otherwise(F.col(\"rating\"))\n",
    "            )\n",
    "            print(\"  Invalid ratings fixed to 0 ‚úÖ\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Exception in rating stage: {e}\")\n",
    "\n",
    "\n",
    "# ---------------- STAGE 5 ----------------\n",
    "def check_price():\n",
    "    try:\n",
    "        print('_-' * 50)\n",
    "        print('‚û§ Checking price values....')\n",
    "        print('-_' * 50)\n",
    "        print(\"\")\n",
    "\n",
    "        df_negative_price = df.filter(F.col(\"price\") < 0).select(\"name\")\n",
    "        count_neg = df_negative_price.count()\n",
    "        print(f\"  Number of hotels with negative price: {count_neg}\")\n",
    "\n",
    "        if count_neg == 0:\n",
    "            print(\"  All prices are valid ‚úÖ\")\n",
    "        else:\n",
    "            globals()['df'] = df.withColumn(\n",
    "                \"price\",\n",
    "                F.when(F.col(\"price\") < 0, 0).otherwise(F.col(\"price\"))\n",
    "            )\n",
    "            print(\"  Negative prices fixed to 0 ‚úÖ\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Exception in price stage: {e}\")\n",
    "\n",
    "\n",
    "# ---------------- STAGE 6 ----------------\n",
    "def split_images():\n",
    "    try:\n",
    "        print('_-' * 50)\n",
    "        print('‚û§ Splitting images into 5 columns....')\n",
    "        print('-_' * 50)\n",
    "        print(\"\")\n",
    "\n",
    "        globals()['df'] = df.withColumn(\"images_array\", F.split(F.col(\"images\"), \",\"))\n",
    "\n",
    "        for i in range(5):\n",
    "            globals()['df'] = df.withColumn(f\"image{i+1}\", F.col(\"images_array\")[i])\n",
    "\n",
    "        print(\"  Image columns created successfully ‚úÖ\")\n",
    "        df.select(\"name\", \"image1\", \"image2\", \"image3\", \"image4\", \"image5\").show(3, truncate=False)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Exception in images stage: {e}\")\n",
    "\n",
    "\n",
    " #======= RUN NEW STAGES WITH TIMING =======\n",
    "#log_time(\"Checking hotel_name\", check_hotel_name)\n",
    "#log_time(\"Creating GPS link\", create_gps_link)\n",
    "#log_time(\"Extracting amenities\", extract_amenities)\n",
    "#log_time(\"Checking rating\", check_rating)\n",
    "#log_time(\"Checking price\", check_price)\n",
    "#log_time(\"Splitting images\", split_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cd5136",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "170a44ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================\n",
      "‚è© File name : hotels_full_data_c2.xlsx\n",
      "==============================\n",
      "\n",
      "hotels_full_data_c2.xlsx Spark Read file .... ‚åõ\n",
      "hotels_full_data_c2.xlsx Spark Read DONE ‚úÖ\n",
      "\n",
      "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-\n",
      "‚û§ Checking hotel_name....\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
      "‚è±Ô∏è  Start time: 21:50:34\n",
      "\n",
      "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-\n",
      "‚û§ Checking hotel_name....\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
      "\n",
      "1-Spacing....\n",
      "\n",
      "  Number of Spaced names : 0\n",
      "  NO FIXED...‚úÖ\n",
      "\n",
      "‚úÖ End time: 21:50:34\n",
      "üïí Duration: 0.15 seconds\n",
      "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-\n",
      "\n",
      "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-\n",
      "‚û§ Creating GPS link....\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
      "‚è±Ô∏è  Start time: 21:50:34\n",
      "\n",
      "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-\n",
      "‚û§ Creating GPS link for each location....\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
      "\n",
      "  GPS link column created successfully ‚úÖ\n",
      "  Sample links:\n",
      "+-------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------+\n",
      "|location                                                           |gps_link                                                                                                           |\n",
      "+-------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------+\n",
      "|Estrada Monumental 139, Funchal, Madeira 9000-098 Portugal         |https://www.google.com/maps/search/?api=1&query=Estrada+Monumental+139,+Funchal,+Madeira+9000-098+Portugal         |\n",
      "|Rua Rosa Araujo 8, Lisbon 1250-195 Portugal                        |https://www.google.com/maps/search/?api=1&query=Rua+Rosa+Araujo+8,+Lisbon+1250-195+Portugal                        |\n",
      "|Rua De Belem 28, Lisbon 1300-084 Portugal                          |https://www.google.com/maps/search/?api=1&query=Rua+De+Belem+28,+Lisbon+1300-084+Portugal                          |\n",
      "|Avenida Tom√É¬°s Cabreira, Praia da Rocha, Portimao 8500-802 Portugal|https://www.google.com/maps/search/?api=1&query=Avenida+Tom√É¬°s+Cabreira,+Praia+da+Rocha,+Portimao+8500-802+Portugal|\n",
      "|Rua da Junqueira 65, Lisbon 1300-343 Portugal                      |https://www.google.com/maps/search/?api=1&query=Rua+da+Junqueira+65,+Lisbon+1300-343+Portugal                      |\n",
      "+-------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "‚úÖ End time: 21:50:35\n",
      "üïí Duration: 0.14 seconds\n",
      "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-\n",
      "\n",
      "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-\n",
      "‚û§ Extracting amenities....\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
      "‚è±Ô∏è  Start time: 21:50:35\n",
      "\n",
      "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-\n",
      "‚û§ Extracting amenities....\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
      "\n",
      "  Total unique amenities found: 262\n",
      "  Amenity columns created successfully ‚úÖ\n",
      "\n",
      "‚úÖ End time: 21:50:46\n",
      "üïí Duration: 11.84 seconds\n",
      "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-\n",
      "\n",
      "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-\n",
      "‚û§ Checking rating....\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
      "‚è±Ô∏è  Start time: 21:50:46\n",
      "\n",
      "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-\n",
      "‚û§ Checking rating values....\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
      "\n",
      "  Number of invalid ratings (<0 or >5): 2\n",
      "  Invalid ratings fixed to 0 ‚úÖ\n",
      "\n",
      "‚úÖ End time: 21:50:47\n",
      "üïí Duration: 0.78 seconds\n",
      "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-\n",
      "\n",
      "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-\n",
      "‚û§ Checking price....\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
      "‚è±Ô∏è  Start time: 21:50:47\n",
      "\n",
      "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-\n",
      "‚û§ Checking price values....\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
      "\n",
      "  Number of hotels with negative price: 0\n",
      "  All prices are valid ‚úÖ\n",
      "\n",
      "‚úÖ End time: 21:50:48\n",
      "üïí Duration: 0.63 seconds\n",
      "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-\n",
      "\n",
      "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-\n",
      "‚û§ Splitting images....\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
      "‚è±Ô∏è  Start time: 21:50:48\n",
      "\n",
      "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-\n",
      "‚û§ Splitting images into 5 columns....\n",
      "-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_\n",
      "\n",
      "  Image columns created successfully ‚úÖ\n",
      "+---------------------------------------+---------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------+\n",
      "|name                                   |image1                                                                                                               |image2                                                                                                           |image3                                                                                                          |image4                                                                                                    |image5                                                                                                    |\n",
      "+---------------------------------------+---------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------+\n",
      "|Reid's Palace, A Belmond Hotel, Madeira|NULL                                                                                                                 |NULL                                                                                                             |NULL                                                                                                            |NULL                                                                                                      |NULL                                                                                                      |\n",
      "|PortoBay Liberdade                     |https://dynamic-media-cdn.tripadvisor.com/media/photo-o/15/b9/52/c8/portobay-liberdade.jpg?w=500&h=-1&s=1            | https://dynamic-media-cdn.tripadvisor.com/media/photo-o/07/68/2f/86/suite.jpg?w=500&h=-1&s=1                    | https://dynamic-media-cdn.tripadvisor.com/media/photo-o/24/d3/42/83/portobay-liberdade-deck7.jpg?w=500&h=-1&s=1| https://dynamic-media-cdn.tripadvisor.com/media/photo-o/23/fd/d4/e1/portobay-liberdade.jpg?w=500&h=-1&s=1| https://dynamic-media-cdn.tripadvisor.com/media/photo-o/1c/e4/07/62/portobay-liberdade.jpg?w=500&h=-1&s=1|\n",
      "|Pensao Residencial Setubalense         |https://dynamic-media-cdn.tripadvisor.com/media/photo-o/05/31/27/88/pensao-residencial-setubalense.jpg?w=500&h=-1&s=1| https://dynamic-media-cdn.tripadvisor.com/media/photo-o/15/a5/df/58/cozy-room-under-the-eaves.jpg?w=500&h=-1&s=1| https://dynamic-media-cdn.tripadvisor.com/media/photo-o/15/a5/df/8e/view-toward-the-river.jpg?w=500&h=-1&s=1   | https://dynamic-media-cdn.tripadvisor.com/media/photo-o/14/5c/a0/cd/photo2jpg.jpg?w=500&h=-1&s=1         | https://dynamic-media-cdn.tripadvisor.com/media/photo-o/14/5c/a0/cc/photo1jpg.jpg?w=500&h=-1&s=1         |\n",
      "+---------------------------------------+---------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "\n",
      "‚úÖ End time: 21:50:49\n",
      "üïí Duration: 1.44 seconds\n",
      "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-\n",
      "\n",
      "hotels_full_data_c2.xlsx Successfully Processed ‚úÖ\n",
      "______________________________\n",
      "hotels_full_data_c2_silver Store in HDFS .... ‚åõ\n",
      "hotels_full_data_c2_silver Successfully Stored ‚úÖ\n",
      "_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-_-\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for file_name in files:\n",
    "\n",
    "    if 'full_data' in file_name:\n",
    "        print(\"=\"*30)\n",
    "        print(f\"‚è© File name : {file_name}\")\n",
    "        print(\"=\"*30)\n",
    "        print(\"\")\n",
    "\n",
    "        try:\n",
    "            print(f\"{file_name} Spark Read file .... ‚åõ\")\n",
    "            df = spark.read \\\n",
    "                .format(\"com.crealytics.spark.excel\") \\\n",
    "                .option(\"header\", \"true\") \\\n",
    "                .option(\"inferSchema\", \"true\") \\\n",
    "                .load(shared_path + '/' + file_name)\n",
    "            print(f\"{file_name} Spark Read DONE ‚úÖ\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error reading file {file_name}: {e}\")\n",
    "            continue  # skip this file if reading fails\n",
    "\n",
    "        #======= RUN NEW STAGES WITH TIMING =======\n",
    "        log_time(\"Checking hotel_name\", check_hotel_name)\n",
    "        log_time(\"Creating GPS link\", create_gps_link)\n",
    "        log_time(\"Extracting amenities\", extract_amenities)\n",
    "        log_time(\"Checking rating\", check_rating)\n",
    "        log_time(\"Checking price\", check_price)\n",
    "        log_time(\"Splitting images\", split_images)\n",
    "\n",
    "        print(f\"{file_name} Successfully Processed ‚úÖ\")\n",
    "        print(\"_\"*30)\n",
    "\n",
    "        # Prepare silver file name\n",
    "        silver_file_name = file_name[0:-5] + '_silver'\n",
    "\n",
    "        try:\n",
    "            print(f\"{silver_file_name} Store in HDFS .... ‚åõ\")\n",
    "            df.write.mode(\"overwrite\").parquet(f\"hdfs://namenode:9000/silver/{silver_file_name}.parquet\")\n",
    "            print(f\"{silver_file_name} Successfully Stored ‚úÖ\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error writing {silver_file_name} to HDFS: {e}\")\n",
    "\n",
    "        print(\"_-\"*50)\n",
    "        print(\"\")\n",
    "\n",
    "\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13811c1",
   "metadata": {},
   "source": [
    "## TRANSFER DATA TO MY LOCAL PC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fdbdc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SaveParquetToCSV\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e49cce2",
   "metadata": {},
   "source": [
    "## merge all reviews in one table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c80208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully written CSV to /data/hotels_full_data_c2_silver_csv\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(\"hdfs://namenode:9000/silver/hotels_reviews_data_*.parquet\")\n",
    "\n",
    "# Drop the column you don't need\n",
    "df = df.drop(\"images_array\")\n",
    "\n",
    "# Write as single CSV file\n",
    "output_path = \"/data/hotels_reviews_data_v_silver_csv\"\n",
    "df.coalesce(1).write \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .csv(output_path)\n",
    "\n",
    "print(f\"‚úÖ Successfully written CSV to {output_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e113a3fb",
   "metadata": {},
   "source": [
    "## merge all hotels data in one table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1548569",
   "metadata": {},
   "source": [
    "to merge 2 files must be same but there are files have properties and other not so we extract all anemities columns put it in list and mapping to every file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e08afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Merge completed and CSV written successfully\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "spark = SparkSession.builder.appName(\"HotelsDataMerge\").getOrCreate()\n",
    "\n",
    "\n",
    "df1 = spark.read.parquet(\"hdfs://namenode:9000/silver/hotels_full_data_c1_silver.parquet\")\n",
    "df2 = spark.read.parquet(\"hdfs://namenode:9000/silver/hotels_full_data_c2_silver.parquet\")\n",
    "\n",
    "core_columns = [\n",
    "    \"name\", \"location\", \"amenities\", \"rating\", \"price\",\n",
    "    \"images\", \"gps_link\", \"image1\", \"image2\", \"image3\", \"image4\", \"image5\"\n",
    "]\n",
    "\n",
    "\n",
    "all_columns = list(set(df1.columns) | set(df2.columns))\n",
    "\n",
    "extra_columns = [c for c in all_columns if c not in core_columns]\n",
    "\n",
    "def add_missing_extra_columns(df, extra_cols):\n",
    "    for c in extra_cols:\n",
    "        if c not in df.columns:\n",
    "            df = df.withColumn(c, lit(0))\n",
    "\n",
    "    return df.select(core_columns + sorted(extra_cols))\n",
    "\n",
    "\n",
    "df1 = add_missing_extra_columns(df1, extra_columns)\n",
    "df2 = add_missing_extra_columns(df2, extra_columns)\n",
    "df1=df1.drop(\"images_array\")\n",
    "df2=df2.drop(\"images_array\")\n",
    "\n",
    "df_merged = df1.unionByName(df2)\n",
    "\n",
    "\n",
    "\n",
    "df_merged.coalesce(1).write.option(\"header\", \"true\").mode(\"overwrite\").csv(\"/data/hotels_full_data_c_silver_csv\")\n",
    "\n",
    "print(\"‚úÖ Merge completed and CSV written successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4332162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (2.0.3)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m178.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting torch\n",
      "  Downloading torch-2.9.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (4.66.1)\n",
      "Collecting plotly\n",
      "  Downloading plotly-6.5.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /opt/conda/lib/python3.11/site-packages (from pandas) (1.24.4)\n",
      "Collecting filelock (from transformers)\n",
      "  Downloading filelock-3.20.0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2025.11.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m628.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting typing-extensions>=4.10.0 (from torch)\n",
      "  Downloading typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /opt/conda/lib/python3.11/site-packages (from torch) (3.2)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /opt/conda/lib/python3.11/site-packages (from torch) (2023.9.2)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.10.2.21 (from torch)\n",
      "  Downloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cublas-cu12==12.8.4.1 (from torch)\n",
      "  Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch)\n",
      "  Downloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.9.90 (from torch)\n",
      "  Downloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch)\n",
      "  Downloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch)\n",
      "  Downloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.7.1 (from torch)\n",
      "  Downloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
      "Collecting nvidia-nccl-cu12==2.27.5 (from torch)\n",
      "  Downloading nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvshmem-cu12==3.3.20 (from torch)\n",
      "  Downloading nvidia_nvshmem_cu12-3.3.20-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.8.90 (from torch)\n",
      "  Downloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch)\n",
      "  Downloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==3.5.1 (from torch)\n",
      "  Downloading triton-3.5.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting narwhals>=1.15.1 (from plotly)\n",
      "  Downloading narwhals-2.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.34.0->transformers)\n",
      "  Downloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2023.7.22)\n",
      "Downloading transformers-4.57.1-py3-none-any.whl (12.0 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m243.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.9.1-cp311-cp311-manylinux_2_28_x86_64.whl (899.8 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m899.8/899.8 MB\u001b[0m \u001b[31m497.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:19\u001b[0mm\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m845.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:01\u001b[0m00:10\u001b[0mm\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m706.8/706.8 MB\u001b[0m \u001b[31m628.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:01\u001b[0m00:14\u001b[0mm\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m915.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:01\u001b[0mm00:06\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m917.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m837.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:01\u001b[0m00:07\u001b[0mm\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:06\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m287.2/287.2 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:05\u001b[0mm\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.3 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m322.3/322.3 MB\u001b[0m \u001b[31m820.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:01\u001b[0m00:07\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvshmem_cu12-3.3.20-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (124.7 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m124.7/124.7 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:03\u001b[0mm\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m851.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading triton-3.5.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (170.4 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m170.4/170.4 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:03\u001b[0mm\n",
      "\u001b[?25hDownloading plotly-6.5.0-py3-none-any.whl (9.9 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading narwhals-2.12.0-py3-none-any.whl (425 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m425.0/425.0 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2025.11.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (800 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m800.4/800.4 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m485.8/485.8 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m787.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading filelock-3.20.0-py3-none-any.whl (16 kB)\n",
      "Downloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-cusparselt-cu12, typing-extensions, triton, sympy, safetensors, regex, nvidia-nvtx-cu12, nvidia-nvshmem-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, narwhals, hf-xet, filelock, plotly, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, huggingface-hub, tokenizers, nvidia-cusolver-cu12, transformers, torch\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.8.0\n",
      "    Uninstalling typing_extensions-4.8.0:\n",
      "      Successfully uninstalled typing_extensions-4.8.0\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.12\n",
      "    Uninstalling sympy-1.12:\n",
      "      Successfully uninstalled sympy-1.12\n",
      "Successfully installed filelock-3.20.0 hf-xet-1.2.0 huggingface-hub-0.36.0 narwhals-2.12.0 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-cu12-9.10.2.21 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-cusparselt-cu12-0.7.1 nvidia-nccl-cu12-2.27.5 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvshmem-cu12-3.3.20 nvidia-nvtx-cu12-12.8.90 plotly-6.5.0 regex-2025.11.3 safetensors-0.6.2 sympy-1.14.0 tokenizers-0.22.1 torch-2.9.1 transformers-4.57.1 triton-3.5.1 typing-extensions-4.15.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas transformers torch tqdm plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4ade88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.11/site-packages (4.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade typing-extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda60d51",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/requests/compat.py:11\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mchardet\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'chardet'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# ÿ™ÿ´ÿ®Ÿäÿ™ ÿßŸÑŸÖŸÉÿ™ÿ®ÿßÿ™ ŸÑŸà ŸÖÿ¥ ŸÖŸàÿ¨ŸàÿØÿ© ŸÅŸä ÿßŸÑŸÉŸàŸÜÿ™ŸäŸÜÿ±\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# !pip install pandas transformers torch tqdm plotly\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müöÄ Starting Python Post-Processing...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/__init__.py:27\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Check the dependencies satisfy the minimal versions required.\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dependency_versions_check\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     29\u001b[0m     OptionalDependencyNotAvailable,\n\u001b[1;32m     30\u001b[0m     _LazyModule,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m     is_pretty_midi_available,\n\u001b[1;32m     37\u001b[0m )\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Note: the following symbols are deliberately exported with `as`\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# so that mypy, pylint or other static linters can recognize them,\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# given that they are not exported using `__all__` in this file.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/dependency_versions_check.py:16\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The HuggingFace Team. All rights reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdependency_versions_table\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deps\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m require_version, require_version_core\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# define which module versions we always want to check at run time\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# (usually the ones defined in `install_requires` in setup.py)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# order specific notes:\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# - tqdm must be checked before tokenizers\u001b[39;00m\n\u001b[1;32m     25\u001b[0m pkgs_to_check_at_runtime \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtqdm\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyyaml\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     38\u001b[0m ]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/utils/__init__.py:19\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#!/usr/bin/env python\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Copyright 2021 The HuggingFace Inc. team. All rights reserved.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfunctools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m lru_cache\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhuggingface_hub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_full_repo_name  \u001b[38;5;66;03m# for backward compatibility\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhuggingface_hub\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstants\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HF_HUB_DISABLE_TELEMETRY \u001b[38;5;28;01mas\u001b[39;00m DISABLE_TELEMETRY  \u001b[38;5;66;03m# for backward compatibility\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpackaging\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m version\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1229\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/huggingface_hub/__init__.py:1044\u001b[0m, in \u001b[0;36m_attach.<locals>.__getattr__\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m   1042\u001b[0m submod_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpackage_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr_to_modules[name]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1043\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1044\u001b[0m     submod \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubmod_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1045\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1046\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError importing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubmod_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/huggingface_hub/hf_api.py:50\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     32\u001b[0m     TYPE_CHECKING,\n\u001b[1;32m     33\u001b[0m     Any,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     46\u001b[0m     overload,\n\u001b[1;32m     47\u001b[0m )\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01murllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m quote\n\u001b[0;32m---> 50\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPError\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauto\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm \u001b[38;5;28;01mas\u001b[39;00m base_tqdm\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/requests/__init__.py:45\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01murllib3\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RequestsDependencyWarning\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcharset_normalizer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__ \u001b[38;5;28;01mas\u001b[39;00m charset_normalizer_version\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/requests/exceptions.py:9\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03mrequests.exceptions\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m~~~~~~~~~~~~~~~~~~~\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03mThis module contains the set of Requests' exceptions.\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01murllib3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m BaseHTTPError\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m CompatJSONDecodeError\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mRequestException\u001b[39;00m(\u001b[38;5;167;01mIOError\u001b[39;00m):\n\u001b[1;32m     13\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"There was an ambiguous exception that occurred while handling your\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;03m    request.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/requests/compat.py:13\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mchardet\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcharset_normalizer\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mchardet\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# -------\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Pythons\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# -------\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Syntax sugar.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/charset_normalizer/__init__.py:24\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mCharset-Normalizer\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m~~~~~~~~~~~~~~\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;03m:license: MIT, see LICENSE for more details.\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m from_bytes, from_fp, from_path, is_binary\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m detect\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CharsetMatch, CharsetMatches\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/charset_normalizer/api.py:5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PathLike\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BinaryIO, List, Optional, Set, Union\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcd\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      6\u001b[0m     coherence_ratio,\n\u001b[1;32m      7\u001b[0m     encoding_languages,\n\u001b[1;32m      8\u001b[0m     mb_encoding_languages,\n\u001b[1;32m      9\u001b[0m     merge_coherence_ratios,\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstant\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IANA_SUPPORTED, TOO_BIG_SEQUENCE, TOO_SMALL_SEQUENCE, TRACE\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmd\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mess_ratio\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/charset_normalizer/cd.py:14\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Counter \u001b[38;5;28;01mas\u001b[39;00m TypeCounter, Dict, List, Optional, Tuple\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstant\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      8\u001b[0m     FREQUENCIES,\n\u001b[1;32m      9\u001b[0m     KO_NAMES,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m     ZH_NAMES,\n\u001b[1;32m     13\u001b[0m )\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmd\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_suspiciously_successive_range\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CoherenceMatches\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     17\u001b[0m     is_accentuated,\n\u001b[1;32m     18\u001b[0m     is_latin,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m     unicode_range,\n\u001b[1;32m     22\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/charset_normalizer/md.py:10\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m List, Optional\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstant\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      6\u001b[0m     COMMON_SAFE_ASCII_CHARACTERS,\n\u001b[1;32m      7\u001b[0m     TRACE,\n\u001b[1;32m      8\u001b[0m     UNICODE_SECONDARY_RANGE_KEYWORD,\n\u001b[1;32m      9\u001b[0m )\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     11\u001b[0m     is_accentuated,\n\u001b[1;32m     12\u001b[0m     is_case_variable,\n\u001b[1;32m     13\u001b[0m     is_cjk,\n\u001b[1;32m     14\u001b[0m     is_emoticon,\n\u001b[1;32m     15\u001b[0m     is_hangul,\n\u001b[1;32m     16\u001b[0m     is_hiragana,\n\u001b[1;32m     17\u001b[0m     is_katakana,\n\u001b[1;32m     18\u001b[0m     is_latin,\n\u001b[1;32m     19\u001b[0m     is_punctuation,\n\u001b[1;32m     20\u001b[0m     is_separator,\n\u001b[1;32m     21\u001b[0m     is_symbol,\n\u001b[1;32m     22\u001b[0m     is_thai,\n\u001b[1;32m     23\u001b[0m     is_unprintable,\n\u001b[1;32m     24\u001b[0m     remove_accent,\n\u001b[1;32m     25\u001b[0m     unicode_range,\n\u001b[1;32m     26\u001b[0m )\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mMessDetectorPlugin\u001b[39;00m:\n\u001b[1;32m     30\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m    Base abstract class used for mess detection plugins.\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;124;03m    All detectors MUST extend and implement given methods.\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/charset_normalizer/utils.py:137\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCJK\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m character_name\n\u001b[0;32m--> 137\u001b[0m \u001b[38;5;129;43m@lru_cache\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmaxsize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mUTF8_MAXIMAL_ALLOCATION\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43mis_hiragana\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcharacter\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mtry\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcharacter_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43municodedata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcharacter\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/functools.py:518\u001b[0m, in \u001b[0;36mlru_cache.<locals>.decorating_function\u001b[0;34m(user_function)\u001b[0m\n\u001b[1;32m    514\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m maxsize \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    515\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    516\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected first argument to be an integer, a callable, or None\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 518\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorating_function\u001b[39m(user_function):\n\u001b[1;32m    519\u001b[0m     wrapper \u001b[38;5;241m=\u001b[39m _lru_cache_wrapper(user_function, maxsize, typed, _CacheInfo)\n\u001b[1;32m    520\u001b[0m     wrapper\u001b[38;5;241m.\u001b[39mcache_parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m : {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaxsize\u001b[39m\u001b[38;5;124m'\u001b[39m: maxsize, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtyped\u001b[39m\u001b[38;5;124m'\u001b[39m: typed}\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# üèóÔ∏è PART 4: PYTHON POST-PROCESSING (PANDAS)\n",
    "# ==========================================\n",
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "from transformers import pipeline\n",
    "\n",
    "# ÿ™ÿ´ÿ®Ÿäÿ™ ÿßŸÑŸÖŸÉÿ™ÿ®ÿßÿ™ ŸÑŸà ŸÖÿ¥ ŸÖŸàÿ¨ŸàÿØÿ© ŸÅŸä ÿßŸÑŸÉŸàŸÜÿ™ŸäŸÜÿ±\n",
    "# !pip install pandas transformers torch tqdm plotly\n",
    "\n",
    "print(\"üöÄ Starting Python Post-Processing...\")\n",
    "\n",
    "# --- Helper Function to read Spark CSV Output ---\n",
    "# Spark saves CSVs as folders (e.g. /data/folder_name/part-0000.csv)\n",
    "def read_spark_csv(folder_path):\n",
    "    try:\n",
    "        # Find the .csv file inside the folder\n",
    "        csv_files = glob.glob(os.path.join(folder_path, \"*.csv\"))\n",
    "        if not csv_files:\n",
    "            raise FileNotFoundError(f\"No CSV files found in {folder_path}\")\n",
    "        \n",
    "        # ÿßŸÑŸÉŸàÿØ ÿßŸÑŸÖÿπÿØŸÑ ŸáŸÜÿß: ÿ•ÿ∂ÿßŸÅÿ© on_bad_lines='skip' Ÿà engine='python'\n",
    "        return pd.read_csv(\n",
    "            csv_files[0], \n",
    "            on_bad_lines='skip', \n",
    "            engine='python' # Ÿäÿ≥ÿ™ÿÆÿØŸÖ ŸÖÿ≠ÿ±ŸÉ ÿßŸÑÿ®ÿßŸäÿ´ŸàŸÜ ÿßŸÑÿ£ŸÉÿ´ÿ± ŸÖÿ±ŸàŸÜÿ©\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error reading Spark output from {folder_path}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# ==========================================\n",
    "# üìç STEP 1: MERGE GEOLOCATION DATA\n",
    "# ==========================================\n",
    "print(\"\\nüìç Starting Location Merge...\")\n",
    "\n",
    "# 1. Read Hotel Data (Produced by Spark)\n",
    "# Note: Path is /data/ because it's mapped in Docker\n",
    "hotels_df = read_spark_csv('/data/hotels_full_data_c_silver_csv')\n",
    "print(f\"   Loaded Hotels Data: {len(hotels_df)} rows\")\n",
    "\n",
    "if not hotels_df.empty:\n",
    "    # Clean duplicates\n",
    "    hotels_df = hotels_df.drop_duplicates(subset=['name', 'location'])\n",
    "    \n",
    "    # 2. Read Location Files from /data/location_l_l\n",
    "    loc_path = '/data/location_l_l'\n",
    "    all_loc_files = glob.glob(os.path.join(loc_path, \"*.csv\"))\n",
    "    \n",
    "    loc_list = []\n",
    "    for filename in all_loc_files:\n",
    "        try:\n",
    "            df_temp = pd.read_csv(filename)\n",
    "            loc_list.append(df_temp)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    if loc_list:\n",
    "        concat_location_data = pd.concat(loc_list, ignore_index=True)\n",
    "        \n",
    "        # Keep only necessary columns\n",
    "        cols_to_keep = ['location', 'Latitude', 'Longitude']\n",
    "        if 'OpenCage Note' in concat_location_data.columns:\n",
    "            cols_to_keep.append('OpenCage Note')\n",
    "            \n",
    "        location_clean = concat_location_data[cols_to_keep].drop_duplicates(subset=['location'])\n",
    "        \n",
    "        # 3. Merge\n",
    "        final_hotels = pd.merge(hotels_df, location_clean, on='location', how='left')\n",
    "        \n",
    "        # Save Intermediate result\n",
    "        final_hotels.to_csv('/data/final_data_without_sentiment.csv', index=False)\n",
    "        print(f\"‚úÖ Location Merge Done. Saved /data/final_data_without_sentiment.csv ({len(final_hotels)} rows)\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No location files found in /data/location_l_l. Skipping merge.\")\n",
    "        final_hotels = hotels_df\n",
    "else:\n",
    "    print(\"‚ùå Hotels data is empty. Skipping location merge.\")\n",
    "    final_hotels = pd.DataFrame()\n",
    "\n",
    "# ==========================================\n",
    "# üß† STEP 2: SENTIMENT ANALYSIS (REVIEWS)\n",
    "# ==========================================\n",
    "print(\"\\nüß† Starting Sentiment Analysis...\")\n",
    "\n",
    "# Settings\n",
    "# Input comes from Spark's reviews output\n",
    "reviews_folder = '/data/hotels_reviews_data_v_silver_csv' \n",
    "output_file = '/data/full_reviews_scored.csv'\n",
    "save_every_n_rows = 500\n",
    "\n",
    "# Load Model\n",
    "print(\"   Loading NLP Model...\")\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", \n",
    "                              model=\"nlptown/bert-base-multilingual-uncased-sentiment\", \n",
    "                              truncation=True, \n",
    "                              max_length=512)\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    if pd.isna(text) or str(text).strip() == \"\": return None\n",
    "    try:\n",
    "        result = sentiment_pipeline(str(text))[0]\n",
    "        return int(result['label'].split(' ')[0])\n",
    "    except: return None\n",
    "\n",
    "# Load Data\n",
    "if os.path.exists(output_file):\n",
    "    print(f\"   Resuming from {output_file}...\")\n",
    "    df_reviews = pd.read_csv(output_file)\n",
    "else:\n",
    "    print(f\"   Reading fresh reviews from {reviews_folder}...\")\n",
    "    df_reviews = read_spark_csv(reviews_folder)\n",
    "    df_reviews['sentiment_score'] = None\n",
    "\n",
    "if not df_reviews.empty:\n",
    "    # Identify rows to process\n",
    "    # If column doesn't exist (fresh load), create it\n",
    "    if 'sentiment_score' not in df_reviews.columns:\n",
    "        df_reviews['sentiment_score'] = None\n",
    "        \n",
    "    rows_to_process = df_reviews[df_reviews['sentiment_score'].isna()].index\n",
    "    \n",
    "    print(f\"   Total Reviews: {len(df_reviews)}\")\n",
    "    print(f\"   Reviews to process: {len(rows_to_process)}\")\n",
    "    \n",
    "    if len(rows_to_process) > 0:\n",
    "        processed_count = 0\n",
    "        for index in tqdm(rows_to_process, desc=\"Analysing Sentiment\"):\n",
    "            review_text = df_reviews.at[index, 'text']\n",
    "            score = analyze_sentiment(review_text)\n",
    "            df_reviews.at[index, 'sentiment_score'] = score\n",
    "            \n",
    "            processed_count += 1\n",
    "            if processed_count % save_every_n_rows == 0:\n",
    "                df_reviews.to_csv(output_file, index=False)\n",
    "        \n",
    "        # Final Save\n",
    "        df_reviews.to_csv(output_file, index=False)\n",
    "        print(\"‚úÖ Sentiment Analysis Complete.\")\n",
    "    else:\n",
    "        print(\"   All reviews already analyzed.\")\n",
    "\n",
    "    # ==========================================\n",
    "    # üèÅ STEP 3: FINAL INTEGRATION (HOTELS + SCORES)\n",
    "    # ==========================================\n",
    "    print(\"\\nüèÅ Creating Final Dataset for App...\")\n",
    "    \n",
    "    # Aggregate Sentiment per Hotel\n",
    "    df_clean_reviews = df_reviews.dropna(subset=['sentiment_score'])\n",
    "    df_clean_reviews['sentiment_score'] = df_clean_reviews['sentiment_score'].astype(int)\n",
    "    \n",
    "    # Calculate average rating per hotel\n",
    "    sentiment_summary = df_clean_reviews.groupby('hotel_name')['sentiment_score'].mean().reset_index()\n",
    "    sentiment_summary.rename(columns={'hotel_name': 'name', 'sentiment_score': 'ai_score'}, inplace=True)\n",
    "    \n",
    "    # Merge with Hotels Data\n",
    "    if not final_hotels.empty:\n",
    "        # Merge on Name\n",
    "        # Note: Make sure names match (trimming was done in Spark)\n",
    "        final_dataset = pd.merge(final_hotels, sentiment_summary, on='name', how='left')\n",
    "        \n",
    "        # Fill missing scores with default 4.0\n",
    "        final_dataset['ai_score'] = final_dataset['ai_score'].fillna(4.0)\n",
    "        \n",
    "        # Save FINAL file for the Web App\n",
    "        final_dataset.to_csv('/data/final_data.csv', index=False)\n",
    "        print(f\"üéâ SUCCESS! Final dataset saved to: /data/final_data.csv\")\n",
    "        print(final_dataset.head(3))\n",
    "    else:\n",
    "        print(\"‚ùå Could not create final dataset (Missing Hotels Data)\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå No reviews data found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7aa50674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Final Assembly (Fast Mode using Summary)...\n",
      "\n",
      "üìç [Step 1] Preparing Hotels & Location Data...\n",
      "‚úÖ Hotels & Location merged. Count: 574\n",
      "\n",
      "üß† [Step 2] Injecting Sentiment Scores...\n",
      "üéâ Found Summary File: /data/reviews/hotel_sentiment_summary.csv\n",
      "‚úÖ Loaded Sentiment Scores for 239 hotels.\n",
      "\n",
      "üèÅ [Step 3] Exporting Final Data...\n",
      "üéâ SUCCESS! Data saved to: /data/final_data.csv\n",
      "------------------------------\n",
      "Sample Data:\n",
      "                             name  ai_score\n",
      "0                 Hotel Os Poetas       4.7\n",
      "1                 Villa Margaridi       4.7\n",
      "2                    Hotel F√°tima       4.1\n",
      "3    Harbour Inn Design Townhouse       4.8\n",
      "4  Onyria Quinta da Marinha Hotel       4.7\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# üèóÔ∏è PART 4: FINAL ASSEMBLY (FAST MODE)\n",
    "# ==========================================\n",
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "\n",
    "print(\"üöÄ Starting Final Assembly (Fast Mode using Summary)...\")\n",
    "\n",
    "# --- ŸÖÿ≥ÿßÿ±ÿßÿ™ ÿßŸÑŸÖŸÑŸÅÿßÿ™ (ÿØÿßÿÆŸÑ ÿßŸÑÿØŸàŸÉÿ±) ---\n",
    "# 1. ŸÖŸÑŸÅÿßÿ™ ÿßŸÑŸÅŸÜÿßÿØŸÇ ŸàÿßŸÑŸÑŸàŸÉŸäÿ¥ŸÜ\n",
    "hotels_input_path = '/data/hotels_full_data_c_silver_csv'\n",
    "location_folder = '/data/location_l_l'\n",
    "\n",
    "# 2. ŸÖŸÑŸÅÿßÿ™ ÿßŸÑÿ±ŸäŸÅŸäŸà (ÿßŸÑÿ™ÿµÿ≠Ÿäÿ≠ ŸáŸÜÿß üëá)\n",
    "# ÿ®ŸÖÿß ÿ•ŸÜ ÿßŸÑŸÖŸÑŸÅ ŸÅŸä shared/reviewsÿå ÿßŸÑÿØŸàŸÉÿ± ÿ®Ÿäÿ¥ŸàŸÅŸá ŸÅŸä /data/reviews\n",
    "reviews_summary_path = '/data/reviews/hotel_sentiment_summary.csv' \n",
    "\n",
    "# 3. ÿßŸÑŸÖÿÆÿ±ÿ¨ ÿßŸÑŸÜŸáÿßÿ¶Ÿä\n",
    "final_output_path = '/data/final_data.csv'\n",
    "\n",
    "# --- Helper Function ---\n",
    "def read_spark_csv(folder_path):\n",
    "    try:\n",
    "        csv_files = glob.glob(os.path.join(folder_path, \"*.csv\"))\n",
    "        if not csv_files: return pd.DataFrame()\n",
    "        return pd.read_csv(csv_files[0], on_bad_lines='skip', engine='python')\n",
    "    except: return pd.DataFrame()\n",
    "\n",
    "# ==========================================\n",
    "# üìç STEP 1: PREPARE HOTELS & LOCATION\n",
    "# ==========================================\n",
    "print(\"\\nüìç [Step 1] Preparing Hotels & Location Data...\")\n",
    "\n",
    "hotels_df = read_spark_csv(hotels_input_path)\n",
    "final_hotels = pd.DataFrame()\n",
    "\n",
    "if not hotels_df.empty:\n",
    "    hotels_df = hotels_df.drop_duplicates(subset=['name', 'location'])\n",
    "    \n",
    "    loc_files = glob.glob(os.path.join(location_folder, \"*.csv\"))\n",
    "    loc_dfs = []\n",
    "    for f in loc_files:\n",
    "        try: loc_dfs.append(pd.read_csv(f))\n",
    "        except: pass\n",
    "    \n",
    "    if loc_dfs:\n",
    "        full_loc_df = pd.concat(loc_dfs, ignore_index=True)\n",
    "        cols = ['location', 'Latitude', 'Longitude']\n",
    "        if 'OpenCage Note' in full_loc_df.columns: cols.append('OpenCage Note')\n",
    "        loc_clean = full_loc_df[cols].drop_duplicates(subset=['location'])\n",
    "        \n",
    "        final_hotels = pd.merge(hotels_df, loc_clean, on='location', how='left')\n",
    "        print(f\"‚úÖ Hotels & Location merged. Count: {len(final_hotels)}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No location files found. Using raw hotels data.\")\n",
    "        final_hotels = hotels_df\n",
    "else:\n",
    "    print(\"‚ùå Hotels data is empty! Check Spark output.\")\n",
    "\n",
    "# ==========================================\n",
    "# üß† STEP 2: INJECT SENTIMENT (FROM SUMMARY FILE)\n",
    "# ==========================================\n",
    "print(\"\\nüß† [Step 2] Injecting Sentiment Scores...\")\n",
    "\n",
    "sentiment_df = pd.DataFrame()\n",
    "\n",
    "# ŸÖÿ≠ÿßŸàŸÑÿ© ŸÇÿ±ÿßÿ°ÿ© ŸÖŸÑŸÅ ÿßŸÑŸÖŸÑÿÆÿµ ÿßŸÑÿ¨ÿßŸáÿ≤\n",
    "if os.path.exists(reviews_summary_path):\n",
    "    print(f\"üéâ Found Summary File: {reviews_summary_path}\")\n",
    "    try:\n",
    "        summary_data = pd.read_csv(reviews_summary_path)\n",
    "        \n",
    "        if 'year_int' in summary_data.columns:\n",
    "            sentiment_df = summary_data.groupby('hotel_name')['sentiment_score'].mean().reset_index()\n",
    "        else:\n",
    "            sentiment_df = summary_data[['hotel_name', 'sentiment_score']].copy()\n",
    "            \n",
    "        sentiment_df.rename(columns={'hotel_name': 'name', 'sentiment_score': 'ai_score'}, inplace=True)\n",
    "        print(f\"‚úÖ Loaded Sentiment Scores for {len(sentiment_df)} hotels.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error reading summary file: {e}\")\n",
    "else:\n",
    "    # ÿ™ÿ¥ÿÆŸäÿµ ÿßŸÑÿÆÿ∑ÿ£ ŸÑŸà ÿßŸÑŸÖŸÑŸÅ ŸÑÿ≥Ÿá ŸÖÿ¥ ŸÖŸÇÿ±ÿ¶\n",
    "    print(f\"‚ö†Ô∏è Summary file NOT found at: {reviews_summary_path}\")\n",
    "    print(\"   Debugging: Listing files in /data/reviews/ ...\")\n",
    "    try:\n",
    "        print(os.listdir('/data/reviews'))\n",
    "    except Exception as e:\n",
    "        print(f\"   Error checking folder: {e}\")\n",
    "\n",
    "# ==========================================\n",
    "# üèÅ STEP 3: FINAL MERGE & EXPORT\n",
    "# ==========================================\n",
    "print(\"\\nüèÅ [Step 3] Exporting Final Data...\")\n",
    "\n",
    "if not final_hotels.empty:\n",
    "    final_hotels['name'] = final_hotels['name'].astype(str).str.strip()\n",
    "    \n",
    "    if not sentiment_df.empty:\n",
    "        sentiment_df['name'] = sentiment_df['name'].astype(str).str.strip()\n",
    "        final_dataset = pd.merge(final_hotels, sentiment_df, on='name', how='left')\n",
    "        final_dataset['ai_score'] = final_dataset['ai_score'].fillna(4.0)\n",
    "        final_dataset['ai_score'] = final_dataset['ai_score'].round(1)\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Proceeding without custom AI scores (Using default 4.0).\")\n",
    "        final_dataset = final_hotels.copy()\n",
    "        final_dataset['ai_score'] = 4.0\n",
    "\n",
    "    final_dataset.to_csv(final_output_path, index=False)\n",
    "    \n",
    "    print(f\"üéâ SUCCESS! Data saved to: {final_output_path}\")\n",
    "    print(\"-\" * 30)\n",
    "    print(\"Sample Data:\")\n",
    "    print(final_dataset[['name', 'ai_score']].head(5))\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Critical Error: No hotel data available to save.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
